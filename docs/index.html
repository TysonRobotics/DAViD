<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>DAViD: Data-efficient and Accurate Vision Models from Synthetic Data</title>
    <link rel="shortcut icon" type="image/jpg" href="img/favicon.ico" />
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
</head>

<body>
    <nav class="navbar is-dark" role="navigation" aria-label="main navigation">
        <div class="container is-max-desktop">
            <div class="navbar-brand">
                <a class="navbar-item"
                    href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                    <img src="img/Microsoft-logo.svg" alt="Mixed Reality & AI Lab â€“ Cambridge" style="height: 1.4rem;">
                </a>
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false"
                    data-target="navbarBasicExample">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div id="navbarBasicExample" class="navbar-menu">
                <div class="navbar-start">
                    <a class="navbar-item" href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                        Mixed Reality & AI
                    </a>
                </div>
                <div class="navbar-end">
                    <a class="navbar-item" href="https://iccv.thecvf.com/Conferences/2025">
                        <img class="is-hidden-touch" src="img/iccv-navbar-logo-white.svg" alt="ICCV 2025" style="height: 100%;">
                        <img class="is-hidden-desktop" src="img/iccv-navbar-logo-col.svg" alt="ICCV 2025" style="height: 100%;">
                    </a>
                </div>
            </div>
        </div>
    </nav>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-2 has-text-centered">DAViD</h1>
            <p class="subtitle is-4 has-text-centered">
                Data-efficient and Accurate Vision Models from Synthetic Data
            </p>
            <p class="subtitle is-5 has-text-centered has-text-grey">
                International Conference on
                <br class="is-hidden-tablet">
                Computer Vision 2025
            </p>
            <p class="subtitle is-6 has-text-centered authors" style="line-height: 1.5;">
                <span><a href="https://fatemeh-slh.github.io/">Fatemeh&nbsp;Saleh</a></span>
                <span><a href="https://sadegh-aa.github.io/">Sadegh&nbsp;Aliakbarian</a></span>
                <span><a href="https://chewitt.me/">Charlie&nbsp;Hewitt</a></span>
                <span><a href="https://lohit.dev">Lohit&nbsp;Petikam</a></span>
                <span><a href="mailto:xianxiao@microsoft.com">Xiao-Xian</a></span>
                <span><a href="mailto:acriminisi@microsoft.com">Antonio&nbsp;Criminisi</a></span>
                <span><a href="https://scholar.google.com/citations?user=pDPdQq8AAAAJ">Thomas&nbsp;J&nbsp;Cashman</a></span>
                <span><a href="mailto:tabaltru@microsoft.com">Tadas&nbsp;Baltru&scaron;aitis</a></span>
            </p>
        </div>
        <div class="container is-max-desktop has-text-centered mt-5">
            <a class="button is-rounded is-link is-light mr-2" disabled>
                <span class="icon"><i class="fa fa-file-text"></i></span>
                <span>Paper</span>
            </a>
            <a href="https://arxiv.org/abs/TODO.TODO" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
            </a>
            <a href="https://youtu.be/TODO" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="fa fa-youtube" aria-hidden="true"></i></span>
                <span>Video</span>
            </a>
            <a href="https://github.com/microsoft/DAViD" class="button is-rounded is-link is-light">
                <span class="icon"><i class="fa fa-github"></i></span>
                <span>Datasets</span>
            </a>
        </div>
    </section>
    <section>
        <div class="container is-max-desktop">
            <figure class="image is-16by9">
                <iframe class="has-ratio" width="640" height="360" src="https://youtube.com/embed/TODO" frameborder="0"
                    allowfullscreen=""></iframe>
            </figure>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Abstract
            </h1>
            <div class="content has-text-justified-desktop">
                The state of the art in human-centric computer vision achieves high accuracy and robustness across a
                diverse range of tasks.
                The most effective models in this domain have billions of parameters, thus requiring extremely large
                datasets, expensive training regimes, and compute-intensive inference.
                In this paper, we demonstrate that it is possible to train models on much smaller but high-fidelity
                synthetic datasets, with no loss in accuracy and higher efficiency.
                Using synthetic <i>training</i> data provides us with excellent levels of detail and perfect labels,
                while providing strong guarantees for data provenance, usage rights, and user consent.
                Procedural data synthesis also provides us with explicit control on data diversity, that we can use to
                address unfairness in the models we train.
                Extensive quantitative assessment on <i>real</i> input images demonstrates accuracy of our models on
                three dense prediction tasks: depth estimation, surface normal estimation, and soft foreground
                segmentation.
                Our models require only a fraction of the cost of training and inference when compared with foundational
                models of similar accuracy.
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                SynthHuman: Human-centric Synthetic Data
            </h1>
            <div class="content has-text-justified-desktop">
                <p>To train our models, we use <i>exclusively</i> synthetic data. Specifically, we use the data generation
                    pipeline of <a href="https://microsoft.github.io/SynthMoCap/">Hewitt et al.</a> incorporating the
                    updated face model of <a href="https://dl.acm.org/doi/full/10.1145/3681758.3697987">Petikam et
                        al.</a> to create a human-centric synthetic dataset with a high degree of realism, as well as
                    high-fidelity ground-truth annotations.
                    Our SynthHuman dataset contains 300K images of resolution 384&#215;512, covering examples of faces,
                    upper body, and full body scenarios equally.
                    We design SynthHuman such that it is diverse in terms of poses, environments, lighting, and
                    appearances, and not tailored to any specific evaluation set.
                    This allows us to train models that generalize across a range of benchmark datasets, as well as on
                    in-the-wild data.</p>
                <div class="columns is-gapless mb-1">
                    <div class="column is-hidden-mobile">
                        <img src="img/SynthHuman-F.jpg">
                    </div>
                    <div class="column is-hidden-mobile">
                        <img src="img/SynthHuman-UB.jpg">
                    </div>
                    <div class="column">
                        <img src="img/SynthHuman-FB.jpg">
                    </div>
                </div>
                <p>Along with the RGB rendered image, each sample includes soft foreground mask, surface normals,
                    and depth ground-truth annotations, used to train our models. </p>
                <video autoplay="" muted="" loop="" playsinline="">
                    <source src="vid/sx_data.mp4" type="video/mp4">
                </video>
            </div>
            <br>
            <div class="container is-max-desktop">
                <h1 class="title is-4">
                    Architecture
                </h1>
                <div class="content has-text-justified-desktop">
                    <p>
                        We use a single model architecture (with varying number of output channels) to tackle the three
                        dense prediction tasks.
                        We adapt the <a href="https://arxiv.org/abs/2103.13413">dense prediction transformer (DPT)</a>
                        to handle variable input resolutions efficiently. Using a single dataset and a single model
                        architecture allows us to easily train a single model with three convolution heads to perform
                        multiple task learning. This is particularly important to combine soft foreground segmentation
                        with depth and normal estimation, as for human-centric tasks it is needed to separate the human
                        from the background.
                    </p>
                    <video autoplay="" muted="" loop="" playsinline="">
                        <source src="vid/architecture.mp4" type="video/mp4">
                    </video>
                </div>
            </div>

    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">Results</h1>
            <div class="content has-text-justified-desktop">
                <p>Our human-centric dense prediction model delivers high-quality, detailed results while achieving
                    remarkable efficiency, running orders of magnitude faster than competing methods, with inference
                    speeds as low as 21 milliseconds per frame (the large multi-task model on an NVIDIA A100). It
                    reliably captures a wide range of human characteristics under diverse lighting conditions,
                    preserving fine-grained details such as hair strands and subtle facial features. This demonstrates
                    the model's robustness and accuracy in complex, real-world scenarios. </p>
                <div class="columns is-gapless is-multiline">
                    <div class="column is-half-tablet is-one-third-desktop">
                        <video autoplay="" muted="" loop="" playsinline="">
                            <source src="vid/5271501-hd_720_1280_25fps.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="column is-half-tablet is-one-third-desktop">
                        <video autoplay="" muted="" loop="" playsinline="">
                            <source src="vid/6872625-hd_720_1280_24fps.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="column is-half-tablet is-one-third-desktop">
                        <video autoplay="" muted="" loop="" playsinline="">
                            <source src="vid/7330823-hd_720_1280_25fps.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="column is-half-tablet is-one-third-desktop">
                        <video autoplay="" muted="" loop="" playsinline="">
                            <source src="vid/7774623-hd_720_1280_30fps.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="column is-half-tablet is-one-third-desktop">
                        <video autoplay="" muted="" loop="" playsinline="">
                            <source src="vid/Media2.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="column is-half-tablet is-one-third-desktop">
                        <video autoplay="" muted="" loop="" playsinline="">
                            <source src="vid/Media3.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                BibTeX
            </h1>
            <pre>
@inproceedings{saleh2025david,
    title={{DAViD}: Data-efficient and Accurate Vision Models from Synthetic Data},
    author={Saleh, Fatemeh and Aliakbarian, Sadegh and Hewitt, Charlie and Petikam, Lohit and Xiao, Xian and Criminisi, Antonio and Cashman, Thomas J. and Baltru{\v{s}}aitis, Tadas},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    year={2025},
    month={October}
}
</pre>
        </div>
    </section>
    <footer class="footer pb-0">
        <div class="content has-text-centered pb-5">
            <p>
                Work conducted at the <a
                    href=https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge>Mixed Reality & AI
                    Lab &ndash; Cambridge</a>.<br>
                <img src="img/Microsoft-logo-only.svg" class="mt-5" alt="Microsoft" style="height: 2rem;">
            </p>
        </div>
        <div class="footer-links content has-text-centered pt-5 has-text-grey-lighter is-size-7">
            <a href="https://go.microsoft.com/fwlink/?LinkId=521839">Privacy</a>
            <a href="https://go.microsoft.com/fwlink/?LinkID=206977">Terms of Use</a>
            <a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks">Trademarks</a>
            <a href="https://microsoft.com">&copy; Microsoft 2025</a>
        </div>
    </footer>
</body>

<script>
    document.addEventListener('DOMContentLoaded', () => {

        // Get all "navbar-burger" elements
        const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

        // Check if there are any navbar burgers
        if ($navbarBurgers.length > 0) {

            // Add a click event on each of them
            $navbarBurgers.forEach(el => {
                el.addEventListener('click', () => {

                    // Get the target from the "data-target" attribute
                    const target = el.dataset.target;
                    const $target = document.getElementById(target);

                    // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
                    el.classList.toggle('is-active');
                    $target.classList.toggle('is-active');

                });
            });
        }
    });
</script>

</html>